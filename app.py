from flask import Flask, request, jsonify
from flask_cors import CORS
from openai import OpenAI
import numpy as np
import faiss
import json
import os
import logging
from datetime import datetime, timedelta
import sys
import io
import aiohttp
import asyncio
import platform
import requests
import traceback
import logging

# å¼ºåˆ¶è®¾ç½®ç³»ç»Ÿé»˜è®¤ç¼–ç ä¸ºUTF-8
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

app = Flask(__name__)
CORS(app)
app.config['JSON_AS_ASCII'] = False
app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True

# ========== åˆå§‹åŒ–æ—¥å¿—é…ç½® ==========
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('debug.log')
    ]
)
logger = logging.getLogger(__name__)
logger.info("âœ… è„šæœ¬å¼€å§‹æ‰§è¡Œ")
@app.route('/api/check_init')
def check_init():
    """æš´åŠ›æ£€æŸ¥å…¨å±€å˜é‡çŠ¶æ€"""
    return jsonify({
        "faiss_index_exists": faiss_index is not None,
        "knowledge_base_exists": knowledge_base is not None,
        "openai_client_exists": client is not None,
        "current_time": datetime.now().isoformat()
    })

# ========== ç™¾åº¦ERNIEé…ç½® ==========
class ChatConfig:
    def __init__(self):
        # ç™¾åº¦ERNIEé…ç½®
        self.api_key = "qehpo02v3xq42r0lrNy3VpZR"
        self.secret_key = "BmPgo9ghoxdLpHv3IMWEkshF83VoZDVW"
        self.embed_api_url = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/embeddings/embedding-v1"
        self.access_token = None
        self.token_expire = None
        
        # OpenAIé…ç½®
        self.openai_api_key = "sk-f104aed04216406abce806380d6670a3"
        self.openai_base_url = "https://api.deepseek.com"
        
        # æ€§èƒ½å‚æ•°
        self.llm_timeout = 25
        self.retrieve_timeout = 8
        self.max_response_tokens = 100
        
        # æ£€ç´¢å‚æ•°
        self.retrieve_top_k = 10  # ä¸´æ—¶è°ƒå¤§æ£€ç´¢æ•°é‡
        self.similarity_threshold = 0.7
        self.debug_mode = False  # è°ƒè¯•å¼€å…³

         # å¯¹è¯ç®¡ç†
        self.memory_window = 4  # å†å²æ¶ˆæ¯è½®æ¬¡

         # æ–°å¢è°ƒè¯•å‚æ•°
        self.min_similarity = 0.3  # æœ€ä½è®°å½•é˜ˆå€¼
        self.max_similarity = 0.9  # æœ€é«˜è®°å½•é˜ˆå€¼ = 4

config = ChatConfig()

# ========== å…¨å±€ç»„ä»¶ ==========
client = None
knowledge_base = None
faiss_index = None

def initialize_components():
    """åˆå§‹åŒ–å¿…è¦ç»„ä»¶ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    global client, knowledge_base, faiss_index
    
    try:
        # 1. è·å–ç™¾åº¦Access Tokenï¼ˆåŒæ­¥è¯·æ±‚ï¼‰
        auth_url = f"https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id={config.api_key}&client_secret={config.secret_key}"
        token_data = requests.get(auth_url, timeout=10).json()  # æ·»åŠ è¶…æ—¶
        if 'error' in token_data:
            raise ValueError(f"ç™¾åº¦è®¤è¯å¤±è´¥: {token_data.get('error_description')}")
        
        config.access_token = token_data["access_token"]
        config.token_expire = datetime.now() + timedelta(seconds=token_data["expires_in"] - 60)
        logger.info(f"ç™¾åº¦ERNIEè®¤è¯æˆåŠŸï¼ŒTokenæœ‰æ•ˆæœŸè‡³: {config.token_expire}")

        # 2. OpenAIå®¢æˆ·ç«¯
        client = OpenAI(
            api_key=config.openai_api_key,
            base_url=config.openai_base_url
        )
        
        # 3. åŠ è½½çŸ¥è¯†åº“ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
        base_dir = os.path.dirname(os.path.abspath(__file__))
        knowledge_path = os.path.join(base_dir, '1.json')
        with open(knowledge_path, 'r', encoding='utf-8') as f:
            knowledge_base = json.load(f)
            logger.info(f"åŠ è½½çŸ¥è¯†åº“ï¼Œå…±{len(knowledge_base)}æ¡æ•°æ®")
        
        # 4. åŠ è½½å¹¶å½’ä¸€åŒ–å‘é‡
        vectors_path = os.path.join(base_dir, 'knowledge_vectors.npy')
        vectors = np.load(vectors_path)
        faiss.normalize_L2(vectors)
        
        # 5. æ„å»ºFAISSç´¢å¼•
        faiss_index = faiss.IndexFlatIP(vectors.shape[1])
        faiss_index.add(vectors)
        logger.info(f"FAISSç´¢å¼•æ„å»ºå®Œæˆï¼Œç»´åº¦: {vectors.shape[1]}")
        def check_vector_health(vectors):
            """å‘é‡ç³»ç»Ÿå¥åº·è¯Šæ–­"""
            try:
                # 1. æ£€æŸ¥å‘é‡èŒƒæ•°
                norms = np.linalg.norm(vectors, axis=1)
                logger.info(f"å‘é‡èŒƒæ•°èŒƒå›´: {norms.min():.4f}-{norms.max():.4f}")
                
                # 2. æ£€æŸ¥ç´¢å¼•ç±»å‹
                assert faiss_index.metric_type == faiss.METRIC_INNER_PRODUCT, "å¿…é¡»ä½¿ç”¨å†…ç§¯ç´¢å¼•"
                
                # 3. æŠ½æ ·æ£€æŸ¥ç›¸ä¼¼åº¦
                sample = vectors[:5] @ vectors[:5].T
                np.fill_diagonal(sample, np.nan)
                logger.info(f"æ ·æœ¬ç›¸ä¼¼åº¦èŒƒå›´: {np.nanmin(sample):.2f}-{np.nanmax(sample):.2f}")
                
            except Exception as e:
                logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
                raise
                # å¥åº·æ£€æŸ¥
        check_vector_health(vectors)
        
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        raise


#ç›¸ä¼¼åº¦åˆ†å¸ƒåˆ†æ
async def analyze_similarity_distribution():
    """ç³»ç»Ÿå¯åŠ¨æ—¶è‡ªåŠ¨åˆ†æç›¸ä¼¼åº¦åˆ†å¸ƒ"""
    test_queries = [
        "å¸¸è§é—®é¢˜", 
        "æ“ä½œæŒ‡å—",
        "é”™è¯¯è§£å†³æ–¹æ³•",
        "ç³»ç»Ÿè¦æ±‚",
        "å¦‚ä½•ä½¿ç”¨"
    ]
    
    logger.info("å¼€å§‹ç›¸ä¼¼åº¦åˆ†å¸ƒåˆ†æ...")
    for query in test_queries:
        embedding = await get_embeddings(query)
        distances, _ = faiss_index.search(embedding.reshape(1, -1), 50)  # æ£€æŸ¥å‰50ä¸ªç»“æœ
        similarities = 1 - distances[0]
        
        logger.info(
            f"æŸ¥è¯¢: '{query}'\n"
            f"ç›¸ä¼¼åº¦åˆ†å¸ƒ: min={similarities.min():.2f} | "
            f"max={similarities.max():.2f} | "
            f"mean={similarities.mean():.2f}\n"
            f"é«˜äºå½“å‰é˜ˆå€¼({config.similarity_threshold})çš„ç»“æœ: "
            f"{sum(similarities > config.similarity_threshold)}ä¸ª"
        )
async def refresh_access_token():
    """å¼‚æ­¥åˆ·æ–°ç™¾åº¦Token"""
    try:
        auth_url = f"https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id={config.api_key}&client_secret={config.secret_key}"
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
            async with session.get(auth_url) as resp:
                if resp.status != 200:
                    raise ValueError(f"HTTPé”™è¯¯: {resp.status}")
                token_data = await resp.json()
                config.access_token = token_data["access_token"]
                config.token_expire = datetime.now() + timedelta(seconds=token_data["expires_in"] - 60)
                logger.info("ç™¾åº¦Tokenåˆ·æ–°æˆåŠŸ")
    except Exception as e:
        logger.error(f"Tokenåˆ·æ–°å¤±è´¥: {str(e)}")
        config.access_token = None  # å¼ºåˆ¶ä¸‹æ¬¡é‡æ–°åˆå§‹åŒ–

async def get_embeddings(text):
    """ä½¿ç”¨ç™¾åº¦ERNIEè·å–æ–‡æœ¬åµŒå…¥ï¼ˆå¼‚æ­¥å®‰å…¨ç‰ˆï¼‰"""
    try:
        # Tokenæ£€æŸ¥ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        if not config.access_token or (config.token_expire and datetime.now() > config.token_expire):
            await refresh_access_token()
            if not config.access_token:
                raise ValueError("æ— æ³•è·å–æœ‰æ•ˆToken")
        
        # APIè°ƒç”¨
        url = f"{config.embed_api_url}?access_token={config.access_token}"
        payload = json.dumps({"input": [text], "user_id": "rag_system"}, ensure_ascii=False)
        
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5)) as session:
            async with session.post(url, headers={'Content-Type': 'application/json'}, data=payload) as resp:
                if resp.status != 200:
                    raise ValueError(f"HTTPé”™è¯¯: {resp.status}")
                
                data = await resp.json()
                if "error_code" in data:
                    raise ValueError(f"APIé”™è¯¯ {data['error_code']}: {data.get('error_msg')}")
                
                embedding = np.array(data['data'][0]['embedding'])
                return embedding / np.linalg.norm(embedding)  # å½’ä¸€åŒ–
                
    except Exception as e:
        logger.error(f"è·å–embeddingå¤±è´¥: {str(e)}")
        return np.zeros(384)  # è¿”å›é›¶å‘é‡ä¿åº•


# ========== æ£€ç´¢å‡½æ•° ==========
async def retrieve_documents(query):
    try:
        query_embedding = await get_embeddings(query)
        query_embedding = query_embedding.reshape(1, -1).astype('float32')
        faiss.normalize_L2(query_embedding)  # æŸ¥è¯¢å‘é‡ä¹Ÿéœ€å½’ä¸€åŒ–

        similarities, indices = faiss_index.search(query_embedding, 20)  # æ‰©å¤§æ£€ç´¢èŒƒå›´

        # åˆ†æ•°ä¿®æ­£å’Œè¿‡æ»¤
        valid_results = [
            {"text": knowledge_base[idx]["text"], "score": float((score + 1)/2)}  # æ˜ å°„åˆ°[0,1]
            for idx, score in zip(indices[0], similarities[0])
            if score > -0.5  # å®¹å¿éƒ¨åˆ†è´Ÿç›¸å…³
        ]
        # ç¡®ä¿è¿”å›ç»“æ„ä¸€è‡´
        return [{
            "text": str(knowledge_base[idx]["text"]),  # å¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸²
            "score": float(score)
        } for idx, score in zip(indices[0], similarities[0]) if score > config.similarity_threshold]
    except Exception as e:
        logger.error(f"æ£€ç´¢å¤±è´¥: {str(e)}")
        return [{
            "text": str(doc["text"]), 
            "score": 0.0
        } for doc in knowledge_base[:config.retrieve_top_k]]  # ä¿åº•è¿”å›
     
    except Exception as e:
        logger.error(f"æ£€ç´¢å¤±è´¥: {str(e)}")
        return []
# ========== æ ¸å¿ƒè·¯ç”± ==========
@app.route("/api/chat", methods=["POST"])
def chat():
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        data = request.get_json()
        if not data or "userInput" not in data:
            return jsonify({"error": "éœ€è¦æä¾›userInputå‚æ•°"}), 400

        return loop.run_until_complete(async_chat_handler(data))
    except Exception as e:
        logger.error(f"è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        return jsonify({"error": str(e)}), 500
    finally:
        loop.close()


async def async_chat_handler(data):
    start_time = datetime.now()
    user_input = data["userInput"].strip()
    messages = data.get("messages", [])
    
    try:
        # ä¿å­˜åŸå§‹é˜ˆå€¼
        original_threshold = config.similarity_threshold
        
        # é¦–æ¬¡æ£€ç´¢
        retrieved_docs = await retrieve_documents(user_input)
        
        # åŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼ˆç´§æ€¥ä¿®å¤ï¼‰
        if len(retrieved_docs) == 0:
            logger.warning(f"é¦–æ¬¡æ£€ç´¢å¤±è´¥ï¼Œå½“å‰é˜ˆå€¼: {config.similarity_threshold}")
            
            # é€æ­¥é™ä½é˜ˆå€¼ç›´åˆ°è·å¾—ç»“æœæˆ–è¾¾åˆ°æœ€ä½é˜ˆå€¼
            for attempt in range(3):
                new_threshold = max(0.3, config.similarity_threshold - 0.1*(attempt+1))
                logger.warning(f"å°è¯•é™ä½é˜ˆå€¼è‡³: {new_threshold}")
                config.similarity_threshold = new_threshold
                retrieved_docs = await retrieve_documents(user_input)
                
                if len(retrieved_docs) > 0:
                    logger.warning(f"åœ¨é˜ˆå€¼ {new_threshold} ä¸‹æ£€ç´¢åˆ° {len(retrieved_docs)} æ¡ç»“æœ")
                    break
                    
            if len(retrieved_docs) == 0:
                logger.error("å³ä½¿é˜ˆå€¼é™è‡³0.3ä»æ— ç»“æœï¼Œè¿”å›é»˜è®¤çŸ¥è¯†")
                retrieved_docs = [doc["text"] for doc in knowledge_base[:config.retrieve_top_k]]
        
        # æ¢å¤åŸå§‹é˜ˆå€¼ï¼ˆé¿å…å½±å“åç»­è¯·æ±‚ï¼‰
        config.similarity_threshold = original_threshold
        
        # è®°å½•æœ€ç»ˆä½¿ç”¨çš„é˜ˆå€¼å’Œç»“æœæ•°
        logger.info(f"æœ€ç»ˆä½¿ç”¨é˜ˆå€¼: {config.similarity_threshold} | ç»“æœæ•°: {len(retrieved_docs)}")
       
        debug_info = f"[é˜ˆå€¼: {config.similarity_threshold:.2f} | ç»“æœæ•°: {len(retrieved_docs)}]"
        knowledge_str = chr(10).join(
            f"- [ç›¸ä¼¼åº¦: {doc['score']:.2f}] {doc['text'][:100]}..." 
            for doc in retrieved_docs
        )
        
        rag_prompt = f"""ã€å‚è€ƒçŸ¥è¯†ã€‘{debug_info}{knowledge_str}ã€é—®é¢˜ã€‘{user_input}"""
        messages.append({"role": "user", "content": rag_prompt})
        
        # æ§åˆ¶å†å²é•¿åº¦
        if len(messages) > config.memory_window * 2:
            messages = messages[-config.memory_window * 2:]

        # è°ƒç”¨å¤§æ¨¡å‹
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=messages,
            max_tokens=config.max_response_tokens,
            timeout=config.llm_timeout
        )
        
        ai_message = response.choices[0].message
        
        return jsonify({
            "reply": ai_message.content,
            "updatedMessages": messages[:-1] + [
                {"role": "user", "content": user_input},
                {"role": ai_message.role, "content": ai_message.content}
            ],
            "relatedKnowledge": retrieved_docs,
            "status": "success",
            "processing_time": (datetime.now() - start_time).total_seconds(),
            "debug": {  # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                "final_threshold": config.similarity_threshold,
                "retrieved_count": len(retrieved_docs)
            }
        })
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        return jsonify({
            "error": str(e),
            "status": "error",
            "debug": {
                "threshold": config.similarity_threshold,
                "retrieved_docs": []
            }
        })
# ========== è°ƒè¯•è·¯ç”± ========== ï¼ˆæ–°å¢è¿™éƒ¨åˆ†ï¼‰
@app.route("/api/debug_search", methods=["POST"])
async def debug_search():
    """å¯è§†åŒ–è°ƒè¯•æ£€ç´¢è¿‡ç¨‹"""
    data = request.get_json()
    query = data.get("query", "æµ‹è¯•æŸ¥è¯¢")
    
    # 1. è·å–æŸ¥è¯¢å‘é‡
    query_embedding = await get_embeddings(query)
    
    # 2. æ‰§è¡Œæœç´¢
    distances, indices = faiss_index.search(
        query_embedding.reshape(1, -1).astype('float32'),
        config.retrieve_top_k * 3  # è·å–æ›´å¤šç»“æœåˆ†æ
    )
    
    # 3. åˆ†æç»“æœ
    results = []
    for idx, dist in zip(indices[0], distances[0]):
        similarity = 1 - dist
        results.append({
            "rank": len(results) + 1,
            "similarity": float(similarity),
            "text": knowledge_base[idx]["text"][:100] + "...",
            "passed": similarity > config.similarity_threshold
        })
    
    return jsonify({
        "query": query,
        "embedding_shape": query_embedding.shape,
        "threshold": config.similarity_threshold,
        "results": sorted(results, key=lambda x: -x["similarity"])
    })    
# ========== æµ‹è¯•è·¯ç”± ==========
@app.route("/api/test_baidu_embedding", methods=["GET"])
async def test_baidu_embedding():
    """æµ‹è¯•ç™¾åº¦embeddingæœåŠ¡çŠ¶æ€"""
    test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
    try:
        token_url = f"https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id={config.api_key}&client_secret={config.secret_key}"
        token_res = requests.get(token_url)
        if token_res.status_code != 200:
            return jsonify({"error": "Tokenè·å–å¤±è´¥", "detail": token_res.text}), 500
        
        embedding = await get_embeddings(test_text)
        
        return jsonify({
            "status": "success",
            "embedding_shape": embedding.shape,
            "sample_values": embedding[:3].tolist()
        })
    except Exception as e:
        return jsonify({
            "error": str(e),
            "traceback": traceback.format_exc()
        }), 500

@app.route("/api/system_status", methods=["GET"])
def system_status():
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
    return jsonify({
        "status": "running",
        "components": {
            "baidu_token_valid": config.token_expire > datetime.now() if config.access_token else False,
            "knowledge_base_loaded": bool(knowledge_base),
            "faiss_index_ready": bool(faiss_index),
            "openai_client_ready": bool(client)
        },
        "timestamp": datetime.now().isoformat()
    })

if __name__ == "__main__":
    # åŒæ­¥åˆå§‹åŒ–ï¼ˆç»•è¿‡Renderçš„å¼‚æ­¥é™åˆ¶ï¼‰
    print("ğŸ› ï¸ å¼€å§‹å¼ºåˆ¶åŒæ­¥åˆå§‹åŒ–...")
    initialize_components()  # ç¡®ä¿è¿™æ˜¯åŒæ­¥å‡½æ•°
    
    # äºŒæ¬¡éªŒè¯
    assert faiss_index is not None, "FAISSç´¢å¼•åˆå§‹åŒ–å¤±è´¥"
    assert knowledge_base is not None, "çŸ¥è¯†åº“åŠ è½½å¤±è´¥"
    assert client is not None, "OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥"
    print("âœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    # å¯åŠ¨Flask
    app.run(host="0.0.0.0", port=10000)
